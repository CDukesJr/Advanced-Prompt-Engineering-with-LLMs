{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23add31a-fc68-4ad2-92b8-7ae97df812de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TelConnect AI Support Tool - Advanced Prompt Engineering Lab\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Up the Environment and Import Libraries\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load token from env; falls back to your provided token if not set\n",
    "api_token = os.getenv(\"HF_API_TOKEN\", \"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\")\n",
    "\n",
    "print(\"TelConnect AI Support Tool - Advanced Prompt Engineering Lab\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd8edff-0654-403a-a6f8-91a8116e47ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Endpoint: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\n",
      "Authentication: Bearer token configured\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the API Endpoint and Request Headers\n",
    "# Let's use a model that's easier to access without special formatting requirements\n",
    "# Mistral is a good alternative to Llama-2 and easier to use\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Set up headers for the API request\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"API Endpoint: {API_URL}\")\n",
    "print(f\"Authentication: Bearer token configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1f9a76-03fc-41b3-824f-c6a6366e8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: ADVANCED PROMPT ENGINEERING\n",
      "--------------------------------------------------\n",
      "System prompt defined: You are an expert technical assistant.\n",
      "User prompt length: 584 characters\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the Advanced Prompt\n",
    "# Define an advanced prompt with in-context examples and chain-of-thought cues\n",
    "system_prompt = \"You are an expert technical assistant.\"\n",
    "user_prompt = \"\"\"\n",
    "When asked a question about debugging Python code, first, analyze the problem step-by-step and then provide a clear solution.\n",
    "\n",
    "Example:\n",
    "Q: I'm getting a 'TypeError: unsupported operand type(s)' when trying to add a string and an integer.\n",
    "A: First, check where the addition is performed. The error indicates that one operand is a string and the other is an integer.\n",
    "   Make sure both operands are of the same type, or convert one using int() or str() as needed.\n",
    "\n",
    "Now, answer the following:\n",
    "Q: I have a function that returns None, but I expected a number. Why might this be happening?\n",
    "\"\"\"\n",
    "\n",
    "print(\"STEP 3: ADVANCED PROMPT ENGINEERING\")\n",
    "print(\"-\" * 50)\n",
    "print(\"System prompt defined:\", system_prompt)\n",
    "print(\"User prompt length:\", len(user_prompt), \"characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868cd429-21a6-4378-9ddf-79b185dff85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: QUERY FUNCTION DEFINED\n",
      "--------------------------------------------------\n",
      "Function 'query_huggingface_api()' is ready to use\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define the Query Function to Call the Hugging Face API\n",
    "def query_huggingface_api():\n",
    "    try:\n",
    "        # Format for Mistral Instruct model\n",
    "        payload = {\n",
    "            \"inputs\": f\"<s>[INST] {user_prompt} [/INST]\",\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 250,\n",
    "                \"temperature\": 0.3,\n",
    "                \"return_full_text\": False,\n",
    "                \"do_sample\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "\n",
    "        # The structure of the response may vary depending on the model\n",
    "        if isinstance(result, list) and result:\n",
    "            return result[0].get(\"generated_text\", \"\")\n",
    "        else:\n",
    "            return str(result)\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        if response.status_code == 401:\n",
    "            return \"Authentication error: Please check your Hugging Face API token.\"\n",
    "        elif response.status_code == 503:\n",
    "            return \"The model is currently loading. Please try again in a few moments.\"\n",
    "        elif response.status_code == 400:\n",
    "            # Provide more specific guidance for 400 errors\n",
    "            error_message = f\"Bad Request Error: {http_err}. The API rejected our request format.\"\n",
    "\n",
    "            # Try a simpler format as a fallback\n",
    "            try:\n",
    "                simpler_payload = {\n",
    "                    \"inputs\": user_prompt,\n",
    "                    \"parameters\": {\n",
    "                        \"max_length\": 150,\n",
    "                        \"temperature\": 0.3\n",
    "                    }\n",
    "                }\n",
    "                print(\"Trying simpler format...\")\n",
    "                response_retry = requests.post(API_URL, headers=headers, json=simpler_payload)\n",
    "                response_retry.raise_for_status()\n",
    "                result_retry = response_retry.json()\n",
    "                if isinstance(result_retry, list) and result_retry:\n",
    "                    return result_retry[0].get(\"generated_text\", \"\")\n",
    "                else:\n",
    "                    return str(result_retry)\n",
    "            except Exception as retry_err:\n",
    "                return f\"{error_message}\\nRetry also failed: {retry_err}\"\n",
    "        else:\n",
    "            return f\"HTTP error occurred: {http_err}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Hugging Face API: {e}\")\n",
    "        # Fallback response\n",
    "        return \"\"\"\n",
    "        A function might return None instead of a number for several reasons:\n",
    "\n",
    "        1. Missing return statement: If a function doesn't have a return statement in some paths, Python returns None by default.\n",
    "\n",
    "        2. Conditional returns: Your function may have if/else conditions where some paths don't return a value.\n",
    "\n",
    "        3. Error handling: The function might catch exceptions and return None when errors occur.\n",
    "\n",
    "        4. Input validation: The function could return None for invalid inputs.\n",
    "\n",
    "        5. Uninitialized variables: If you're returning a variable that hasn't been assigned a value.\n",
    "\n",
    "        Check these issues by adding print statements or using a debugger to trace execution flow.\n",
    "        \"\"\"\n",
    "\n",
    "print(\"STEP 4: QUERY FUNCTION DEFINED\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Function 'query_huggingface_api()' is ready to use\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215f7bcc-0176-404f-a91c-042c4d5a84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: EXECUTING QUERY AND DISPLAYING OUTPUT\n",
      "--------------------------------------------------\n",
      "Sending request to Hugging Face API...\n",
      "Generated Response: HTTP error occurred: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Execute the Query and Display the Output\n",
    "print(\"STEP 5: EXECUTING QUERY AND DISPLAYING OUTPUT\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Sending request to Hugging Face API...\")\n",
    "\n",
    "# Get response (either from API or fallback)\n",
    "generated_output = query_huggingface_api()\n",
    "print(\"Generated Response:\", generated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f676741-4e07-416e-a2df-2f83d694bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: FIXING MODEL URL AND RETRYING\n",
      "--------------------------------------------------\n",
      "Updated API Endpoint: https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium\n",
      "Retrying with updated model...\n",
      "Generated Response: HTTP error occurred: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Fix Model URL and Retry\n",
    "print(\"STEP 6: FIXING MODEL URL AND RETRYING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Update to a more reliable model\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium\"\n",
    "\n",
    "# Update headers with new URL\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"Updated API Endpoint: {API_URL}\")\n",
    "\n",
    "# Retry the query with the new model\n",
    "print(\"Retrying with updated model...\")\n",
    "generated_output = query_huggingface_api()\n",
    "print(\"Generated Response:\", generated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d007b8d1-1b82-4d43-80a4-6d97f79a226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: TRYING ALTERNATIVE MODEL\n",
      "--------------------------------------------------\n",
      "Trying API Endpoint: https://api-inference.huggingface.co/models/gpt2\n",
      "Sending request to GPT-2 model...\n",
      "Generated Response: Error: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/gpt2\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Try Alternative Model\n",
    "print(\"STEP 7: TRYING ALTERNATIVE MODEL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Try a different, more commonly available model\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "\n",
    "# Update headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"Trying API Endpoint: {API_URL}\")\n",
    "\n",
    "# Modify the query function for GPT-2 format\n",
    "def query_gpt2_api():\n",
    "    try:\n",
    "        # Simpler format for GPT-2\n",
    "        payload = {\n",
    "            \"inputs\": user_prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_length\": 200,\n",
    "                \"temperature\": 0.7,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"Sending request to GPT-2 model...\")\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        if isinstance(result, list) and result:\n",
    "            return result[0].get(\"generated_text\", \"\")\n",
    "        else:\n",
    "            return str(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the new model\n",
    "generated_output = query_gpt2_api()\n",
    "print(\"Generated Response:\", generated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0645aafc-0413-4aff-9d62-35df695ee85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8: VERIFY TOKEN AND TEST CONNECTION\n",
      "--------------------------------------------------\n",
      "Token preview: hf_RcMLCrT...inAoF\n",
      "Token length: 37\n",
      "Testing token validity...\n",
      "Token test status code: 200\n",
      "Token is valid!\n",
      "Authenticated as: Cdukesjr\n",
      "\n",
      "Trying basic text generation model...\n",
      "Basic model status: 404\n",
      "Basic model failed: Not Found\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Verify Token and Test Connection\n",
    "print(\"STEP 8: VERIFY TOKEN AND TEST CONNECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if token is properly set\n",
    "print(f\"Token preview: {api_token[:10]}...{api_token[-5:]}\")\n",
    "print(f\"Token length: {len(api_token)}\")\n",
    "\n",
    "# Test with a simple API call to verify token works\n",
    "test_url = \"https://huggingface.co/api/whoami-v2\"\n",
    "test_headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "try:\n",
    "    print(\"Testing token validity...\")\n",
    "    test_response = requests.get(test_url, headers=test_headers)\n",
    "    print(f\"Token test status code: {test_response.status_code}\")\n",
    "    \n",
    "    if test_response.status_code == 200:\n",
    "        print(\"Token is valid!\")\n",
    "        user_info = test_response.json()\n",
    "        print(f\"Authenticated as: {user_info.get('name', 'Unknown')}\")\n",
    "    else:\n",
    "        print(\"Token validation failed\")\n",
    "        print(f\"Response: {test_response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Token test error: {e}\")\n",
    "\n",
    "# Try the most basic Hugging Face model\n",
    "print(\"\\nTrying basic text generation model...\")\n",
    "basic_url = \"https://api-inference.huggingface.co/models/distilgpt2\"\n",
    "basic_payload = {\"inputs\": \"Hello, this is a test\"}\n",
    "\n",
    "try:\n",
    "    basic_response = requests.post(basic_url, headers=headers, json=basic_payload)\n",
    "    print(f\"Basic model status: {basic_response.status_code}\")\n",
    "    if basic_response.status_code == 200:\n",
    "        print(\"Basic model works!\")\n",
    "        print(f\"Response: {basic_response.json()}\")\n",
    "    else:\n",
    "        print(f\"Basic model failed: {basic_response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Basic model error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01e2f4d-445f-4460-b0fd-9ee168e9d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 9: ADVANCED PROMPT ENGINEERING FOR TELCONNECT\n",
      "============================================================\n",
      "Code samples loaded:\n",
      "- sklearn_categorical: sklearn LinearRegression failing with categorical data\n",
      "- tensorflow_shape: TensorFlow neural network with input shape mismatch\n",
      "- pytorch_dimension: PyTorch model with dimension mismatch between layers\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Complete Advanced Prompt Engineering Lab\n",
    "print(\"STEP 9: ADVANCED PROMPT ENGINEERING FOR TELCONNECT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define the three problematic code samples for debugging\n",
    "CODE_SAMPLES = {\n",
    "    \"sklearn_categorical\": {\n",
    "        \"code\": '''import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"feature\": [\"low\", \"medium\", \"high\", \"medium\"],\n",
    "    \"target\": [1, 2, 3, 2]\n",
    "})\n",
    "\n",
    "X = data[[\"feature\"]]\n",
    "y = data[\"target\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)''',\n",
    "        \"description\": \"sklearn LinearRegression failing with categorical data\",\n",
    "        \"type\": \"sklearn\"\n",
    "    },\n",
    "    \n",
    "    \"tensorflow_shape\": {\n",
    "        \"code\": '''import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(5,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "X = np.random.rand(100, 4)\n",
    "y = np.random.rand(100, 1)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X, y, epochs=5)''',\n",
    "        \"description\": \"TensorFlow neural network with input shape mismatch\",\n",
    "        \"type\": \"tensorflow\"\n",
    "    },\n",
    "    \n",
    "    \"pytorch_dimension\": {\n",
    "        \"code\": '''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(6, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "input_data = torch.randn(3, 10)\n",
    "output = model(input_data)''',\n",
    "        \"description\": \"PyTorch model with dimension mismatch between layers\",\n",
    "        \"type\": \"pytorch\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Code samples loaded:\")\n",
    "for key, sample in CODE_SAMPLES.items():\n",
    "    print(f\"- {key}: {sample['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b70b87b-7b3a-4a02-a9c6-444479c80014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 10: CREATING ADVANCED PROMPT ENGINEERING FUNCTION\n",
      "------------------------------------------------------------\n",
      "Advanced prompt engineering function created successfully!\n",
      "Function includes: Role-based prompting, In-context learning, Chain-of-thought reasoning\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Create Advanced Prompt Engineering Function\n",
    "print(\"STEP 10: CREATING ADVANCED PROMPT ENGINEERING FUNCTION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def create_advanced_debugging_prompt(code_sample, description, code_type):\n",
    "    \"\"\"\n",
    "    Create an advanced prompt with multiple prompt engineering techniques:\n",
    "    - Role-based prompting\n",
    "    - In-context learning (few-shot examples)\n",
    "    - Chain-of-thought reasoning\n",
    "    - Structured output format\n",
    "    \"\"\"\n",
    "    \n",
    "    # Role-based prompting - Define expert persona\n",
    "    system_role = \"\"\"You are a senior machine learning engineer and debugging specialist \n",
    "    working for TelConnect, a telecommunications company. You specialize in identifying \n",
    "    and fixing issues in data science pipelines and machine learning models.\"\"\"\n",
    "    \n",
    "    # In-context learning - Provide debugging example\n",
    "    example_debugging = \"\"\"\n",
    "    Example debugging approach:\n",
    "    \n",
    "    PROBLEM: ValueError in sklearn model training\n",
    "    CODE: model.fit(X_categorical, y)\n",
    "    \n",
    "    ANALYSIS:\n",
    "    1. ISSUE IDENTIFICATION: sklearn models expect numerical input, not categorical strings\n",
    "    2. ROOT CAUSE: Categorical data needs preprocessing before model training\n",
    "    3. SOLUTION: Use LabelEncoder or OneHotEncoder to convert categorical to numerical\n",
    "    4. CORRECTED CODE:\n",
    "       from sklearn.preprocessing import LabelEncoder\n",
    "       le = LabelEncoder()\n",
    "       X_encoded = le.fit_transform(X_categorical)\n",
    "       model.fit(X_encoded.reshape(-1, 1), y)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chain-of-thought instructions\n",
    "    reasoning_steps = \"\"\"\n",
    "    Please analyze the following code using this systematic approach:\n",
    "    1. ISSUE IDENTIFICATION: What error will occur?\n",
    "    2. ROOT CAUSE: Why does this problem happen?\n",
    "    3. SOLUTION STEPS: How to fix it step-by-step?\n",
    "    4. CORRECTED CODE: Show the fixed code with comments\n",
    "    5. PREVENTION: Best practices to avoid this issue\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all elements into advanced prompt\n",
    "    advanced_prompt = f\"\"\"\n",
    "    {system_role}\n",
    "    \n",
    "    {reasoning_steps}\n",
    "    \n",
    "    {example_debugging}\n",
    "    \n",
    "    Now analyze this {code_type} code:\n",
    "    \n",
    "    DESCRIPTION: {description}\n",
    "    \n",
    "    CODE TO DEBUG:\n",
    "    ```python\n",
    "    {code_sample}\n",
    "    ```\n",
    "    \n",
    "    Please provide your systematic analysis following the 5-step approach above.\n",
    "    \"\"\"\n",
    "    \n",
    "    return advanced_prompt\n",
    "\n",
    "print(\"Advanced prompt engineering function created successfully!\")\n",
    "print(\"Function includes: Role-based prompting, In-context learning, Chain-of-thought reasoning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4269afa6-112b-41eb-9a1c-086817db93f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 11: CREATING DEBUGGING EXECUTION FUNCTION\n",
      "------------------------------------------------------------\n",
      "Debugging execution function created successfully!\n",
      "Ready to run debugging sessions on all three code samples\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Create Debugging Execution Function\n",
    "print(\"STEP 11: CREATING DEBUGGING EXECUTION FUNCTION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def execute_debugging_session(code_key):\n",
    "    \"\"\"\n",
    "    Execute a complete debugging session using advanced prompt engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nDEBUGGING SESSION: {code_key.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get the code sample\n",
    "    sample = CODE_SAMPLES[code_key]\n",
    "    \n",
    "    print(f\"Problem: {sample['description']}\")\n",
    "    print(f\"Code Type: {sample['type']}\")\n",
    "    \n",
    "    # Create advanced prompt\n",
    "    advanced_prompt = create_advanced_debugging_prompt(\n",
    "        code_sample=sample['code'],\n",
    "        description=sample['description'], \n",
    "        code_type=sample['type']\n",
    "    )\n",
    "    \n",
    "    print(f\"Advanced prompt created ({len(advanced_prompt)} characters)\")\n",
    "    \n",
    "    # Update the global user_prompt for the API call\n",
    "    global user_prompt\n",
    "    user_prompt = advanced_prompt\n",
    "    \n",
    "    print(\"Sending advanced prompt to AI...\")\n",
    "    \n",
    "    # Get AI response using our working API function\n",
    "    try:\n",
    "        response = query_huggingface_api()\n",
    "        print(\"\\nAI DEBUGGING RESPONSE:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(response)\n",
    "        print(\"-\" * 30)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error during debugging session: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Debugging execution function created successfully!\")\n",
    "print(\"Ready to run debugging sessions on all three code samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dadf218-aa67-4482-a300-eda77a1f4a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 12: EXECUTING DEBUGGING SESSIONS\n",
      "============================================================\n",
      "Running debugging session 1 of 3...\n",
      "\n",
      "DEBUGGING SESSION: SKLEARN_CATEGORICAL\n",
      "==================================================\n",
      "Problem: sklearn LinearRegression failing with categorical data\n",
      "Code Type: sklearn\n",
      "Advanced prompt created (1691 characters)\n",
      "Sending advanced prompt to AI...\n",
      "\n",
      "AI DEBUGGING RESPONSE:\n",
      "------------------------------\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/gpt2\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "PAUSE BETWEEN SESSIONS\n",
      "============================================================\n",
      "Running debugging session 2 of 3...\n",
      "\n",
      "DEBUGGING SESSION: TENSORFLOW_SHAPE\n",
      "==================================================\n",
      "Problem: TensorFlow neural network with input shape mismatch\n",
      "Code Type: tensorflow\n",
      "Advanced prompt created (1711 characters)\n",
      "Sending advanced prompt to AI...\n",
      "\n",
      "AI DEBUGGING RESPONSE:\n",
      "------------------------------\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/gpt2\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "PAUSE BETWEEN SESSIONS\n",
      "============================================================\n",
      "Running debugging session 3 of 3...\n",
      "\n",
      "DEBUGGING SESSION: PYTORCH_DIMENSION\n",
      "==================================================\n",
      "Problem: PyTorch model with dimension mismatch between layers\n",
      "Code Type: pytorch\n",
      "Advanced prompt created (1807 characters)\n",
      "Sending advanced prompt to AI...\n",
      "\n",
      "AI DEBUGGING RESPONSE:\n",
      "------------------------------\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/gpt2\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "ALL DEBUGGING SESSIONS COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Execute Debugging Sessions\n",
    "print(\"STEP 12: EXECUTING DEBUGGING SESSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute debugging session for sklearn categorical data issue\n",
    "print(\"Running debugging session 1 of 3...\")\n",
    "sklearn_result = execute_debugging_session(\"sklearn_categorical\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PAUSE BETWEEN SESSIONS\")\n",
    "print(\"=\"*60)\n",
    "time.sleep(2)  # Brief pause between sessions\n",
    "\n",
    "# Execute debugging session for tensorflow shape issue  \n",
    "print(\"Running debugging session 2 of 3...\")\n",
    "tensorflow_result = execute_debugging_session(\"tensorflow_shape\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PAUSE BETWEEN SESSIONS\") \n",
    "print(\"=\"*60)\n",
    "time.sleep(2)  # Brief pause between sessions\n",
    "\n",
    "# Execute debugging session for pytorch dimension issue\n",
    "print(\"Running debugging session 3 of 3...\")\n",
    "pytorch_result = execute_debugging_session(\"pytorch_dimension\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL DEBUGGING SESSIONS COMPLETED\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573460d9-3ed0-4ff5-af4a-f4017eb2e101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
